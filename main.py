import aiohttp
import asyncio
import os
import re
import json
from datetime import datetime
from urllib.parse import quote
from typing import Dict, Any
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import astrbot.api.message_components as Comp
from astrbot.api.message_components import Node, Plain, Image, Nodes
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api.all import AstrBotConfig
from astrbot.api import logger
from .method import TEMP_DIR, get_img_changeFormat

#è‡ªå®šä¹‰å¼‚å¸¸ç±»
class ExceedSearchLimit(Exception):
    '''æ£€ç´¢åŒ¹é…å†…å®¹è¶…è¿‡ä¸€ç§'''
    pass

class FetchError(Exception):
    '''æŠ“å–é”™è¯¯'''
    pass

class NoAnimeFound(Exception):
    '''æ‰¾ä¸åˆ°å¯¹åº”çš„ç•ªå‰§'''
    pass

#user_agent
ua = UserAgent()


#èœœæŸ‘åŠ¨æ¼«ç£é“¾çˆ¬å–æ–¹æ³•
class Mikan():
    def __init__(self):
        self.base_url = 'https://mikanani.me/'
        self.header = {
            "User-Agent" : ua.random
        }
        self.html = None
        self.proxy = None
        self.title = None
        self.url = None
        self.tracker = None


    #é€šç”¨è¯·æ±‚ç½‘é¡µ
    async def fetch_url(self, url, session):
        base_url = url
        header = self.header
        try:
            async with session.get(base_url, headers=header, proxy=self.proxy) as response:
                response.raise_for_status()
                fetch_html = await response.text()
                html = BeautifulSoup(fetch_html,"html.parser")
                return html
        except Exception as e:
             raise FetchError(f'æŠ“å–{base_url}é”™è¯¯') from e
        
        
    #é€šç”¨å­—å¹•ç»„/æ ‡é¢˜/ç£é“¾å®šä½
    async def sub_title_magnet(self, id):
        start_node = self.html.find('div', class_= f'subgroup-scroll-top-{id}')
        html_between = []
        try:
            for sibling in start_node.find_next_siblings():
                if f'subgroup-scroll-end-{id}' in sibling.get('class', []):
                    break
                html_between.append(sibling)
            
            return html_between

        except Exception:
            raise        

    
    #è¯¦æƒ…é¡µé¢å¤„ç†
    async def html_magnet(self, subid, html:BeautifulSoup):
        subname = []
        magnet = []
        output = []
        if (html[-1]).get('data-take'):
            pattern = r'(\d+)$'
            bgmid = re.search(pattern, self.url)
            more_link = f'https://mikanani.me/Home/ExpandEpisodeTable?bangumiId={bgmid.group(1)}&subtitleGroupId={subid}&take=115'
            try:
                async with aiohttp.ClientSession() as session:
                    more_mgt = await self.fetch_url(more_link, session)
            except Exception:
                raise
            subname_list = more_mgt.find_all('a',class_ = "magnet-link-wrap")
            magnet_list = more_mgt.find_all('a',class_ = "js-magnet magnet-link")

        else:
            subname_list = html[-1].find_all('a',class_ = "magnet-link-wrap")
            magnet_list = html[-1].find_all('a',class_ = "js-magnet magnet-link")
        for sn in subname_list:
            subname.append(sn.text)
        for mgt in magnet_list:
            mgt = mgt.get("data-clipboard-text")
            mgt_parts = mgt.split('&', 1)
            mgt = mgt_parts[0]
            self.tracker = f'&{mgt_parts[1]}'
            magnet.append(mgt)
        for c in range(len(subname)):
            output.append(f'{subname[c]}:\n{magnet[c]}\n')
        return output

    def re_quality(self, mgt_list):
        quality_mgt = []
        regex = r'\b1080[pP]\b'
        for mgt in mgt_list:
            if re.search(regex, mgt):
                quality_mgt.append(mgt)
        
        return quality_mgt





    #æŸ¥æ‰¾åŒ¹é…åŠ¨æ¼«  
    async def search_anime_count(self, proxy_set, keyword, session):
        q_keyword = quote(keyword)
        base_url = f'https://mikanani.me/Home/Search?searchstr={q_keyword}'
        header = self.header
        self.proxy = proxy_set
        try:
            async with session.get(base_url, headers=header, proxy=self.proxy) as response:
                response.raise_for_status()
                res_html = await response.text()
                html = BeautifulSoup(res_html,"html.parser")
                title_list = html.find_all('div',class_ = 'an-text')

                #å¤„ç†å¹¶è¿”å›æ£€ç´¢ç»“æœ
                if len(title_list) == 1:
                    self.title = (title_list[0])['title']
                    url = html.select('ul.list-inline.an-ul > li > a[target="_blank"]')
                    self.url = url[0]
                    return self.title
                
                elif len(title_list) == 0:
                    raise NoAnimeFound(f'æ‰¾ä¸åˆ°å¯¹åº”çš„ç•ªå‰§ï¼Œè¯·æ›´æ”¹æœç´¢è¯ï¼Œç¼©çŸ­åç§°ï¼Œä½¿ç”¨å…¨ç§°ï¼Œæˆ–è€…å»é™¤ç¬¦å·æ¥è¿›è¡Œæœç´¢')
                
                else:
                    #å¤„ç†ç±»ä¼¼â€œxxxxâ€å’Œâ€œxxxx ç¬¬xå­£"ä¹‹é—´æ— æ³•åŒºåˆ†çš„é—®é¢˜
                    default = False
                    title = []
                    for l in title_list:
                        if keyword == l.text:
                            self.title = l.text
                            url = html.select('ul.list-inline.an-ul > li > a[target="_blank"]')
                            for n, t in enumerate(url):
                                if l.text == (t.find_all('div',class_ = 'an-text'))[0].text:
                                    self.url = url[n]
                                    break
                            default = True
                            return self.title

                    if default is False:
                        for l in title_list:
                            title.append(l['title'])
                            out_put = '\n'.join(title)
                        raise ExceedSearchLimit(f'æ‰¾åˆ°å¤šä¸ªç•ªå‰§ï¼Œè¯·é‡æ–°é€‰æ‹©:\n{out_put}')
                    
                                            
        except Exception:
            raise

    #è·å–ç•ªå‰§å°é¢å›¾ç‰‡
    async def get_anime_image(self):
        p_link = (self.html.find_all('div',class_ = 'bangumi-poster'))[0].get('style')
        pattern = re.compile(r"url\('(.*?)\?")
        link = (pattern.search(p_link)).group(1)
        img_link = f'https://mikanani.me{link}'

        try:
            imgpath = await get_img_changeFormat(img_link, self.proxy, TEMP_DIR)
            with open(imgpath,"rb") as img:
                img_byte = img.read()
            
            #æ¸…ç†ç¼“å­˜å›¾ç‰‡
            if os.path.exists(imgpath):
                try:
                    os.remove(imgpath)
                except Exception as e:
                    logger.error(f"åˆ é™¤è½¬æ¢åå›¾ç‰‡ç¼“å­˜æ–‡ä»¶å¤±è´¥ï¼Œé”™è¯¯åŸå› :{e}")

            return img_byte       
        
        except Exception:
            raise
    
    #è·å–ç•ªå‰§åŸºç¡€ä¿¡æ¯
    def get_anime_info(self):
        info = self.html.find_all('p',class_ = 'bangumi-info')
        bgm_info = self.html.find_all('a',class_ = 'w-other-c')
        bgm_link  = bgm_info[1].text 
        ever_data = info[0].text 
        output = f'{ever_data}\nBgmé“¾æ¥ï¼š{bgm_link}'

        return output

    
    #å¤„ç†å¹¶æ•´åˆç£é“¾ç»“æœ
    async def get_search_magnet(self):  
        self.url = (self.url)['href'] 
        base_url = f'https://mikanani.me{self.url}'

        try:
            async with aiohttp.ClientSession() as session:
                self.html:BeautifulSoup = await self.fetch_url(base_url, session)
        except Exception:
                raise        

        try:
            #è·å–å­—å¹•ç»„åå•
            try:
                subgroup = {}
                id_list= []
                sub = self.html.find_all('a', class_ = 'subgroup-name')
                if sub:
                    for group in sub:
                        subname = group.text
                        id = (group.get('data-anchor')).removeprefix('#')
                        id_list.append(id)
                        subgroup[id] = subname
                else:
                    raise Exception(f"å­—å¹•ç»„åå•è·å–å¤±è´¥ï¼Œä¸ºç©º") 
            except Exception:
                raise           

            #å¹¶å‘æ‰§è¡Œè·å–ä»»åŠ¡
            tasks = []
            try:
                for id in id_list:
                    subid = id
                    task = asyncio.create_task(self.sub_title_magnet(subid))
                    tasks.append(task)
                    op_html = await asyncio.gather(*tasks)

            except Exception:
                raise

            finally:
                for task in tasks:
                    if not task.done():
                        task.cancel()
                logger.info(f'å¹¶å‘è·å–ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆ')
            
            #å¤„ç†è·å–ç£åŠ›é“¾ç»“æœ
            results = {}
            for i, html in enumerate(op_html):
                subid = id_list[i]
                sub_name = subgroup.get(subid)
                name_mgt = await self.html_magnet(subid, html)
                quality_mgt = self.re_quality(name_mgt)
                results[sub_name] = quality_mgt
            
            return self.tracker, results
        
        except Exception:
            raise

    #æ•´ç†ä¿¡æ¯è¾“å‡ºæ ¼å¼
    def output_format(self):
        info = self.get_anime_info()
        output = f'{self.title}\n{info}'

        return output




@register("Animemagnet", "Hxfrzc", "è¿™æ˜¯ä¸€ä¸ªåŠ¨æ¼«ç£é“¾è·å–æ’ä»¶", "1.0")
class bt_getter(Star):
    def __init__(self, context: Context, config:AstrBotConfig):
        super().__init__(context)
        self.context_config = self.context.get_config()
        self.config = config

        #è·å–ä»£ç†é…ç½®
        self.cmd_config_path = os.path.join(self.context.get_config().get("data_dir", "data"),  f"cmd_config.json")
        with open(self.cmd_config_path, 'r', encoding='utf-8-sig') as f:
            set_proxy = (json.load(f)).get("http_proxy")

        #å¦‚æœæ²¡æœ‰è®¾ç½®ä»£ç†é€‰æ‹©astrbotè®¾ç½®çš„å…¨å±€ä»£ç†
        self.proxy = self.config.get('proxy', None)
        if not self.proxy:
            self.proxy = set_proxy

        self.mk = Mikan()

    
    @filter.command("bt")
    async def bt(self, event: AstrMessageEvent):
        """
        è·å–æŒ‡å®šåŠ¨æ¼«çš„ç£åŠ›é“¾æ¥ï¼Œ
        ä½¿ç”¨æ–¹æ³•  /bt [åŠ¨æ¼«åç§°]
        """ 
        word = event.message_str.split(maxsplit=1)
        if len(word)<2:
            yield event.plain_result("å‚æ•°é”™è¯¯ï¼Œè¯·è¾“å…¥ç•ªå‰§ï¼Œä¾‹å­ï¼š/bt è¯·é—®ä½ ä»Šå¤©è¦æ¥ç‚¹å…”å­å—")
            return
        
        keyword = word[1]
        yield event.plain_result(f"æ­£åœ¨æœç´¢ï¼š{keyword}")

        try:
            #å¤„ç†æŸ¥è¯¢ç»“æœ
            try:
                async with aiohttp.ClientSession() as session:
                    title = await self.mk.search_anime_count(self.proxy, keyword, session)
                    yield event.plain_result(f"åŒ¹é…åˆ°åŠ¨æ¼«ï¼š{title}ï¼Œæ­£åœ¨è¿”å›ç£åŠ›é“¾")

            except NoAnimeFound as e:
                yield event.plain_result(f'{e}')

            except ExceedSearchLimit as e:
                yield event.plain_result(f"{e}")
            
            except Exception as e:
                logger.error(f'å¤„ç†å¤±è´¥ï¼Œé”™è¯¯åŸå› {e}')
            
            #å¤„ç†æŸ¥è¯¢åˆ°çš„ç£é“¾
            try:
                trakcer, name_bt = await self.mk.get_search_magnet()
                subname_list = list(name_bt.keys())
                subname = '\n'.join(subname_list)

            except Exception as e:
                logger.error(f'è·å–ç£é“¾å¤±è´¥ï¼Œé”™è¯¯åŸå› {e}')

            info = self.mk.output_format() 
            chain_list = []
            info_chains = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Image.fromBytes(await self.mk.get_anime_image()),
                    Plain(f'{info}\n'),
                ]
            )
            tracker_chains = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Plain(f'Trackerï¼š\n{trakcer}'),
                ]
            )
            sub_chains = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Plain(f'å­—å¹•ç»„åå•ï¼š\n{subname}'),
                ]
            )
            chain_list.append(info_chains)
            chain_list.append(sub_chains)
            chain_list.append(tracker_chains)

            for c in range(len(name_bt)):
                if not name_bt[subname_list[c]]:
                    continue

                for m in range(len(name_bt[subname_list[c]])):
                    mgt_chains = Node(
                        uin=3974507586,
                        name="ç–ç–ç‘ ",
                        content=[
                            Plain(f'å­—å¹•ç»„ â†’ ã€Œ{subname_list[c]}ã€:\nğŸ§²ï¼š\n{name_bt[subname_list[c]][m]}')
                        ]
                    )
                    chain_list.append(mgt_chains)

                if len(name_bt[subname_list[c]]) >5:
                    break

            chain_obj = Nodes(nodes=chain_list)


            yield event.chain_result([chain_obj])    
        
        except Exception as e:
            logger.error(f'å‡ºé”™ï¼Œé”™è¯¯åŸå› {e}')

        
    @filter.command("btn")
    async def btn(self, event: AstrMessageEvent):
        """
        è·å–æŒ‡å®šåŠ¨æ¼«æœ€æ–°çš„ç£åŠ›é“¾æ¥ï¼Œ
        ä½¿ç”¨æ–¹æ³•  /btn [åŠ¨æ¼«åç§°]
        """ 
        word = event.message_str.split(maxsplit=1)
        if len(word)<2:
            yield event.plain_result("å‚æ•°é”™è¯¯ï¼Œè¯·è¾“å…¥ç•ªå‰§ï¼Œä¾‹å­ï¼š/btn è¯·é—®ä½ ä»Šå¤©è¦æ¥ç‚¹å…”å­å—")
            return
        
        keyword = word[1]
        yield event.plain_result(f"æ­£åœ¨æœç´¢ï¼š{keyword}")

        try:
            #å¤„ç†æŸ¥è¯¢ç»“æœ
            try:
                async with aiohttp.ClientSession() as session:
                    title = await self.mk.search_anime_count(self.proxy, keyword, session)
                    yield event.plain_result(f"åŒ¹é…åˆ°åŠ¨æ¼«ï¼š{title}ï¼Œæ­£åœ¨è¿”å›æœ€æ–°è·å–çš„ç£åŠ›é“¾")

            except NoAnimeFound as e:
                yield event.plain_result(f'{e}')

            except ExceedSearchLimit as e:
                yield event.plain_result(f"{e}")
            
            except Exception as e:
                logger.error(f'å¤„ç†å¤±è´¥ï¼Œé”™è¯¯åŸå› {e}')
            
            #å¤„ç†æŸ¥è¯¢åˆ°çš„ç£é“¾
            try:
                trakcer, name_bt = await self.mk.get_search_magnet()
                subname_list = list(name_bt.keys())
                subname = '\n'.join(subname_list)

            except Exception as e:
                logger.error(f'è·å–ç£é“¾å¤±è´¥ï¼Œé”™è¯¯åŸå› {e}')

            info = self.mk.output_format() 
            chain_list = []
            info_chains = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Image.fromBytes(await self.mk.get_anime_image()),
                    Plain(f'{info}\n'),
                ]
            )
            tracker_chains = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Plain(f'Trackerï¼š\n{trakcer}'),
                ]
            )
            sub_chains = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Plain(f'å­—å¹•ç»„åå•ï¼š\n{subname}'),
                ]
            )
            chain_list.append(info_chains)
            chain_list.append(sub_chains)
            chain_list.append(tracker_chains)
            for m in range(len(subname_list)):
                sub_title = Node(
                uin=3974507586,
                name="ç–ç–ç‘ ",
                content=[
                    Plain(f'ã€Œ{subname_list[m]}ã€:'),
                ]
            )
                mgt_chains = Node(
                    uin=3974507586,
                    name="ç–ç–ç‘ ",
                    content=[
                        Node(
                            uin=3974507586,
                            name="ç–ç–ç‘ ",
                            content=[
                            Plain(f'å­—å¹•ç»„ â†’ ã€Œ{subname_list[m]}ã€:\nğŸ§²ï¼š\n{name_bt[subname_list[m]][0]}')
                            ]
                        )
                    ]
                )
                chain_list.append(sub_title)
                chain_list.append(mgt_chains)
            chain_obj = Nodes(nodes=chain_list)


            yield event.chain_result([chain_obj]) 

        except Exception as e:
            logger.error(f'å‡ºé”™ï¼Œé”™è¯¯åŸå› {e}')       
            


    async def terminate(self):
        """å¯é€‰æ‹©å®ç°å¼‚æ­¥çš„æ’ä»¶é”€æ¯æ–¹æ³•ï¼Œå½“æ’ä»¶è¢«å¸è½½/åœç”¨æ—¶ä¼šè°ƒç”¨ã€‚"""
        pass
